{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d3f0c76",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch transformers datasets dotenv scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fa021c",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d2d33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 256\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6628bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salehafzoon/Desktop/PersoChat/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertModel, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13fb7654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 35077 examples with 105 relation types.\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data/ConvAI2/u2t_map_all.json\"\n",
    "with open(data_path, \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# Extract all unique relations\n",
    "all_relations = sorted({ex[\"triplets\"][0][\"label\"] for ex in raw_data})\n",
    "relation2id = {rel: i for i, rel in enumerate(all_relations)}\n",
    "id2relation = {i: rel for rel, i in relation2id.items()}\n",
    "print(f\"Loaded {len(raw_data)} examples with {len(relation2id)} relation types.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a57c073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bio(example):\n",
    "    triplet = example[\"triplets\"][0]\n",
    "    tokens = triplet[\"tokens\"]\n",
    "    head = triplet[\"head\"]\n",
    "    tail = triplet[\"tail\"]\n",
    "    relation = triplet[\"label\"]\n",
    "\n",
    "    labels = ['O'] * len(tokens)\n",
    "    for idx in head:\n",
    "        labels[idx] = 'B-SUB'\n",
    "    if isinstance(tail, list):\n",
    "        for i, idx in enumerate(tail):\n",
    "            labels[idx] = 'B-OBJ' if i == 0 else 'I-OBJ'\n",
    "\n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"labels\": labels,\n",
    "        \"relation_label\": relation2id[relation]\n",
    "    }\n",
    "\n",
    "bio_data = [convert_to_bio(ex) for ex in raw_data if \"triplets\" in ex and ex[\"triplets\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5661afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "label_list = ['O', 'B-SUB', 'B-OBJ', 'I-OBJ']\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "def tokenize_and_align(example):\n",
    "    encoding = tokenizer(example[\"tokens\"], is_split_into_words=True, padding=\"max_length\", truncation=True, max_length= MAX_LENGTH)\n",
    "    word_ids = encoding.word_ids()\n",
    "\n",
    "    aligned_labels = []\n",
    "    prev_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_id != prev_word_id:\n",
    "            aligned_labels.append(label2id[example[\"labels\"][word_id]])\n",
    "        else:\n",
    "            aligned_labels.append(label2id[example[\"labels\"][word_id]] if 'I' in example[\"labels\"][word_id] else label2id['O'])\n",
    "        prev_word_id = word_id\n",
    "\n",
    "    encoding[\"labels\"] = aligned_labels\n",
    "    encoding[\"relation_label\"] = example[\"relation_label\"]\n",
    "    return encoding\n",
    "\n",
    "tokenized_data = [tokenize_and_align(ex) for ex in bio_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06ce8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointPersonaDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.data = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(item[\"input_ids\"]),\n",
    "            \"attention_mask\": torch.tensor(item[\"attention_mask\"]),\n",
    "            \"labels\": torch.tensor(item[\"labels\"]),\n",
    "            \"relation_label\": torch.tensor(item[\"relation_label\"])\n",
    "        }\n",
    "\n",
    "dataset = JointPersonaDataset(tokenized_data)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "def39b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointBertExtractor(nn.Module):\n",
    "    def __init__(self, base_model='bert-base-uncased', num_token_labels=4, num_relation_labels=0):\n",
    "        super(JointBertExtractor, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(base_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.token_classifier = nn.Linear(self.bert.config.hidden_size, num_token_labels)\n",
    "        self.relation_classifier = nn.Linear(self.bert.config.hidden_size, num_relation_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None, relation_label=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "        cls_output = self.dropout(outputs.pooler_output)\n",
    "\n",
    "        token_logits = self.token_classifier(sequence_output)  # [batch_size, seq_len, num_token_labels]\n",
    "        relation_logits = self.relation_classifier(cls_output)  # [batch_size, num_relation_labels]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None and relation_label is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            token_loss = loss_fct(token_logits.view(-1, token_logits.shape[-1]), labels.view(-1))\n",
    "            relation_loss = loss_fct(relation_logits, relation_label)\n",
    "            loss = token_loss + relation_loss\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"token_logits\": token_logits,\n",
    "            \"relation_logits\": relation_logits\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4664a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = JointBertExtractor(\n",
    "    num_token_labels=len(label2id),\n",
    "    num_relation_labels=len(relation2id)\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "847135a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  get_scheduler\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = EPOCHS\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20e417a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/1974 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1974/1974 [03:45<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 2.2909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1974/1974 [03:48<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 1.2030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1974/1974 [03:48<00:00,  8.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.7514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1974/1974 [03:49<00:00,  8.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.4748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1974/1974 [03:48<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.2989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1974/1974 [03:47<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — Train Loss: 0.1930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1974/1974 [03:47<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 — Train Loss: 0.1175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1974/1974 [03:48<00:00,  8.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 — Train Loss: 0.0785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1974/1974 [03:48<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 — Train Loss: 0.0499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 1974/1974 [03:48<00:00,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 — Train Loss: 0.0345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"],\n",
    "            relation_label=batch[\"relation_label\"]\n",
    "        )\n",
    "\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} — Train Loss: {avg_train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a15e253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to PExtractor\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"PExtractor\"\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "torch.save(model.state_dict(), os.path.join(model_save_path, \"pytorch_model.bin\"))\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c4924b",
   "metadata": {},
   "source": [
    "# Load and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7cd9fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salehafzoon/Desktop/PersoChat/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast\n",
    "from torch import nn\n",
    "from transformers import BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d02d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointBertExtractor(nn.Module):\n",
    "    def __init__(self, base_model='bert-base-uncased', num_token_labels=4, num_relation_labels=0):\n",
    "        super(JointBertExtractor, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(base_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.token_classifier = nn.Linear(self.bert.config.hidden_size, num_token_labels)\n",
    "        self.relation_classifier = nn.Linear(self.bert.config.hidden_size, num_relation_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None, relation_label=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "        cls_output = self.dropout(outputs.pooler_output)\n",
    "\n",
    "        token_logits = self.token_classifier(sequence_output)\n",
    "        relation_logits = self.relation_classifier(cls_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None and relation_label is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            token_loss = loss_fct(token_logits.view(-1, token_logits.shape[-1]), labels.view(-1))\n",
    "            relation_loss = loss_fct(relation_logits, relation_label)\n",
    "            loss = token_loss + relation_loss\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"token_logits\": token_logits,\n",
    "            \"relation_logits\": relation_logits\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69e7f46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JointBertExtractor(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (token_classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (relation_classifier): Linear(in_features=768, out_features=105, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"PExtractor\"  # adjust if saved elsewhere\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model = JointBertExtractor(num_token_labels=4, num_relation_labels=105)  # 105 = your # of relation classes\n",
    "model.load_state_dict(torch.load(f\"{model_path}/pytorch_model.bin\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd1e7746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token-level labels\n",
    "label_list = ['O', 'B-SUB', 'B-OBJ', 'I-OBJ']\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "# Relation labels (recreate from training set)\n",
    "import json\n",
    "with open(\"data/ConvAI2/u2t_map_all.json\", \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "relation_list = sorted({ex[\"triplets\"][0][\"label\"] for ex in raw_data})\n",
    "id2relation = {i: rel for i, rel in enumerate(relation_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d514c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triplet_joint(sentence: str, model, tokenizer, id2label, id2relation, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "    token_preds = torch.argmax(outputs[\"token_logits\"], dim=-1).squeeze().cpu().tolist()\n",
    "    tokens_decoded = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze())\n",
    "    attention_mask = inputs[\"attention_mask\"].squeeze().cpu().tolist()\n",
    "\n",
    "    subject = None\n",
    "    obj_tokens = []\n",
    "\n",
    "    for token, label_id, mask in zip(tokens_decoded, token_preds, attention_mask):\n",
    "        if mask == 0 or token in [\"[PAD]\", \"[CLS]\", \"[SEP]\"]:\n",
    "            continue  # skip padding tokens\n",
    "        label = id2label.get(label_id, \"O\")\n",
    "        if label == \"B-SUB\":\n",
    "            subject = token\n",
    "        elif label.startswith(\"B-OBJ\") or label.startswith(\"I-OBJ\"):\n",
    "            obj_tokens.append(token)\n",
    "\n",
    "\n",
    "    rel_pred_id = torch.argmax(outputs[\"relation_logits\"], dim=-1).item()\n",
    "    relation = id2relation[rel_pred_id]\n",
    "    object_str = tokenizer.convert_tokens_to_string(obj_tokens).strip()\n",
    "    subject = subject if subject else \"i\"\n",
    "\n",
    "    return (subject, relation, object_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecebddaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Triplet: (i, favorite_movie, horror)\n"
     ]
    }
   ],
   "source": [
    "test_sent = \"I just got done watching a horror movie.\"\n",
    "s, r, o = extract_triplet_joint(test_sent, model, tokenizer, id2label, id2relation, device)\n",
    "print(f\"Extracted Triplet: ({s}, {r}, {o})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46463d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
